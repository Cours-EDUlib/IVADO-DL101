{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tutorial 4 rnn - solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B9ALu78bhPXM"
      },
      "source": [
        "# IVADO/MILA DEEP LEARNING SCHOOL\n",
        "# 5th edition (Winter 2019)\n",
        "# Tutorial : Recurrent neural networks (RNNs)\n",
        "## Tutorial was adapted from this one (in French): \n",
        "https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/RNN/RNN_solutions.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MIH_PfZV1rNa"
      },
      "source": [
        "## Authors\n",
        "\n",
        "Francis Grégoire <francis.gregoire@mila.quebec>\n",
        "\n",
        "Jeremy Pinto <jeremy.pinto@mila.quebec>\n",
        "\n",
        "Mirko Bronzi <mirko.bronzi@mila.quebec>\n",
        "\n",
        "Arsène Fansi Tchango <arsene.fansi.tchango@mila.quebec>\n",
        "\n",
        "### Translation to English: \n",
        "\n",
        "Laurent Charlin <lcharlin@gmail.com>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OYrPFO1p1rX1"
      },
      "source": [
        "## Preface\n",
        "\n",
        "This tutorial introduces the fundamental concepts that underlie recurrent neural networks (RNN et LSTM) using two example tasks.\n",
        "\n",
        "The first task builds and compares an LSTM model to an RNN model.\n",
        "\n",
        "In the second task, we exploit the properties of an LSTM by developing a neural language model to generate text. Through this example, you will learn how to preprocess text data to efficiently train a neural language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ArGnixElhPXN"
      },
      "source": [
        "---\n",
        "# Initialization \n",
        "\n",
        "To ensure that this tutorial runs properly on the Colab environment, we must install a few libraries using the `pip` utility. \n",
        "\n",
        "To begin, ensure that you are \"connected\" to the notebook ( check for \"✓ CONNECTED\" at the top right of your window). Then execute the cell below by selecting it and clicking `shift`+`Enter`. You will see the version of the installed `PyTorch` package and whether or not a GPU is available on this runtime session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UwebZdYMhPXT",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import time\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "# Setting the seed to a fixed value can be helpful in reproducing results\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "print(\"PyTorch version: \", torch.__version__)\n",
        "print(\"GPU available: {}\".format(use_gpu))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SLcPxj5z2vX"
      },
      "source": [
        "---\n",
        "# Task 1: Adding numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Y-PnqdMhPXX"
      },
      "source": [
        "\n",
        "## Objective\n",
        "\n",
        "Our goal is to build a recurrent model capable of adding an arbitrary series of numbers. It is all in all a fairly simple task (any pocket calculator can trivially do it) yet it will demonstrate that this procedure can be learned from data. Further, we will use it to outline certain limitations of RNNs compared to LSTMs. It is also a good first task since it will be easy to generate data for it and train our models (both RNN and LSTM) on this data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nihkB--rz6WL"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Our data is composed of $x$ a set of sequences of numbers of length $seq\\_len$ each associated with a target $y$ the sum of each element in each $x$. In other words, the input to our model will be a vector $\\mathbf x^{(i)} = \\left[x_{1}^{i}, x_{2}^{i}, \\dots, x_{T}^{i}\\right]$ of length $seq\\_len=T$ and a target $y^{(i)}$ given by:\n",
        "\n",
        "\\begin{align}  \n",
        "y^{(i)}=\\sum_{j=1}^{seq\\_len}x^{(i)}_j,\n",
        "\\end{align}\n",
        "\n",
        "where $j$ indexes time.\n",
        "\n",
        "For example, for the input $\\mathbf x^{(i)}$ with $seq\\_len=4$, we have:\n",
        "\n",
        "\\begin{align}  \n",
        "\\mathbf x^{(i)} &= \\left[ 4,-1,15,24\\right], \\, \\mathbf x^{(i)} \\in \\mathbb R^{4}; \\\\ \n",
        "y^{(i)} &= 42, \\, \\mathrm y^{(i)} \\in \\mathbb R.\n",
        "\\end{align}\n",
        "\n",
        "We will use our data to train both an RNN and an LSTM. Since the target is an unbounded integer, we will use a linear hidden layer (i.e., identity activation) to project the last recurrent state of the RNN/LSTM, $h^{(i)}_{T}$, as shown on the following figure:\n",
        "\n",
        "![alt-text](https://github.com/Cours-EDUlib/IVADO-DL101/blob/master/images/rnn_sum.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zZNrVvS8hPXY"
      },
      "source": [
        "### Generating our dataset\n",
        "\n",
        "It will be useful to have a function that can generate random datasets of `n_samples` sequences each of length `seq_length`. To do so we will rely on the function [torch.randint()](https://pytorch.org/docs/stable/torch.html#torch.randint). The function `generate_data` takes as input the following arguments:\n",
        "- **n_samples** (int): number of sequences to generate.\n",
        "- **seq_len** (int): length of each sequence.\n",
        "- **input_dim** (int, optional): dimension of the input data. Default: 1.\n",
        "- **xmin** (float, optional): minimum possible value in the sequence. Default: -100.\n",
        "- **xmax** (float, optional): maximum possible value in the sequence. Default: -100.\n",
        "\n",
        "It returns a tuple of two elements corresponding respectively to:\n",
        "- **X** ([torch.FloatTensor](https://pytorch.org/docs/stable/tensors.html)): A tensor of shape $n\\_samples \\times seq\\_len \\times input\\_dim$ representing a set of `n_samples` sequences, each of length `seq_len`. The elements of the sequences are of dimension `input_dim`, that is, they belong to ${\\mathbb R}^{input\\_dim}$.\n",
        "- **Y** ([torch.FloatTensor](https://pytorch.org/docs/stable/tensors.html)): A tensor of shape $n\\_samples \\times  input\\_dim$ representing the corresponding sum of the elements for each sequence in `X`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "colab_type": "code",
        "id": "nKFfyWFahPXZ",
        "colab": {}
      },
      "source": [
        "def generate_data(n_samples, seq_len, input_dim=1, xmin=-100, xmax=100):\n",
        "    \"\"\"Generate tensors X and Y within the [xmin, xmax] interval.\n",
        "    \n",
        "    Args : \n",
        "      n_samples: int, number of sequences to generate\n",
        "      seq_len: int, length of each sequence\n",
        "      input_dim: int, dimension of the input data\n",
        "      xmin: minimum possible value in the sequence\n",
        "      xmax: maximum possible value in the sequence\n",
        "    \n",
        "    Returns: n_samples sequence of numbers X and associated targets Y in this\n",
        "             format torch.Tensor where X.shape = (n_samples, seq_len, 1)\n",
        "             and Y.shape = (n_samples, 1).\n",
        "    \"\"\"\n",
        "    X = torch.randint(xmin, xmax+1, (n_samples, seq_len, input_dim))\n",
        "    Y = X.sum(dim=1)\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJdyRkUoj4qI",
        "colab_type": "text"
      },
      "source": [
        "The following piece of code shows an example of how to use the `generate_data` function. In this example, we are generating 1000 sequences of numbers $\\in \\mathbb R$ (i.e, `input_dim` is set to 1), each one of lenght 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URAZtBZxjTXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 1000\n",
        "seq_len = 4\n",
        "input_dim = 1\n",
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "print(\"Tensor dimensions X = {}\".format(X.shape))\n",
        "print(\"where n_samples = {}, seq_len = {}, input_dim = {}\".format(*X.shape))\n",
        "print(\"data example: {}\".format(X[0,:,0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s1wHZ_PFhPXf"
      },
      "source": [
        "### Standardizing data\n",
        "\n",
        "To help the training of our model we will first standardize the input data such that it has, overall, `mean` 0 and standard deviation (`stdev`) 1. To do so we simply calculate the mean and std of our data and then substract the former and divide by the latter. In addition, we will store these two values for later usage. This standardization often speeds up learning.\n",
        "\n",
        "**Note**: since we generated our data from a uniform distribution, the standard deviation should be close to $\\frac{(xmax-xmin)}{\\sqrt{12}}$ while the mean should be close to $\\frac{(xmax+xmin)}{2}$.\n",
        "\n",
        "The following function performs such a standardization operation. It takes as input a tensor **X** (of shape $n\\_samples \\times seq\\_len \\times input\\_dim$) we would like to standardize and returns a tuple of 4 elements corresponding respectively to:\n",
        "- **Xs**: the standardized version of X, of shape $n\\_samples \\times seq\\_len \\times input\\_dim$\n",
        "- **Ys**: the new sum of the sequences of Xs, of shape $n\\_samples \\times input\\_dim$\n",
        "- **mean**: the mean of X, float.\n",
        "- **stdev**: the stdandard deviation of X, float."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UdL_y85mhPXg",
        "colab": {}
      },
      "source": [
        "def standardize(X):\n",
        "    \"\"\"The function standardizes the X tensor. \n",
        "    Args:\n",
        "      X: torch.Tensor.\n",
        "    \n",
        "    Returns:\n",
        "      Xs: torch.Tensor standardize.\n",
        "      Ys: torch.Tensor, the (new) sum of Xs.\n",
        "      mean: float, the mean of X.\n",
        "      stdev: float, the stdev of X.\n",
        "    \"\"\"\n",
        "    \n",
        "    X=X.float()\n",
        "    mean = torch.mean(X)\n",
        "    std = torch.std(X)\n",
        "    Xs = (X-mean) / std\n",
        "    Ys = Xs.sum(dim=1)\n",
        "    \n",
        "    return Xs, Ys, mean, std\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxZa-OJJsrnQ",
        "colab_type": "text"
      },
      "source": [
        "An example of how to use the above defined function is shown below. Here, after generating the data using the `generate_data` function, we use the `standardize` function to standardize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Lw5IutmbmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "example_before = X[0,:,0]\n",
        "Xs, Ys, mean, std = standardize(X)\n",
        "print(\"mean = {:.4f}, std = {:.4f}\".format(mean, std))\n",
        "print('example before standardization: {}'.format(X[0,:,0]),\n",
        "      '\\nexample after standardization: {}'.format(Xs[0,:,0])\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wCm8xalthPXj"
      },
      "source": [
        "## RNN implementation \n",
        "\n",
        "We will define our RNN using the following PyTorch class [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). For more details regarding the implementation of this class, we suggest this [tutorial](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net). Once initialized, it takes input data `X` of shape `(seq_len, batch_size, input_dim)` (recall that we will use `input_dim=1` for our task). As explained earlier, we then add a linear layer to transform the last hidden recurrent state to have the same dimensionality as `Y` which has size `(batch_size, input_dim)` (again `input_dim=1`). \n",
        "\n",
        "To define the architecture of our RNN, we will use this module [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn) followed by a linear layer [torch.nn.Linear()](https://pytorch.org/docs/stable/nn.html#linear). The following methods are to be completed:\n",
        "<ul>\n",
        "<li>The `__init__()` method to define the different layers of our model. </li>\n",
        "<li>The `forward()` method which uses the layers and the input variables and returns an output (this is effectively a *forward pass*).</li>\n",
        "</ul>\n",
        "\n",
        "**NB**: \n",
        "\n",
        "* Remember that only the last output of the RNN is needed, i.e., `output[-1]`.\n",
        "\n",
        "* You must ensure that the dimensions of your input data `X` matches what is required by the RNN class [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). \n",
        "\n",
        "  **Hint:** this method [tensor.transpose()](https://pytorch.org/docs/stable/tensors.html?highlight=transpose#torch.Tensor.transpose) can be useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3cX7eeKxiXi",
        "colab_type": "text"
      },
      "source": [
        "### Exercise:\n",
        "\n",
        "Complete this piece of code to implement the above described network using the [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn) class. The input arguments are the following:\n",
        "- **input_dim**: the dimension of the input data\n",
        "- **output_dim**: the dimension of the output data\n",
        "- **hiden_size**: the size of the hidden state of the RNN\n",
        "- **n_layers**: the number of layers of the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3QCVSZGzhPXk",
        "colab": {}
      },
      "source": [
        "class RNNLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, hidden_size, n_layers):\n",
        "        super(RNNLinear, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim,\n",
        "                          hidden_size,\n",
        "                          n_layers)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # The RNN's input must be of size (seq_len, batch_size, input_dim)\n",
        "        x = x.transpose(0, 1) \n",
        "        output, _ = self.rnn(x)\n",
        "        pred = self.linear(output[-1])\n",
        "        return pred\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBuEIPdxytWp",
        "colab_type": "text"
      },
      "source": [
        "Here is an example of how this class can be used to predict the values of the sequences in Xs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNxsM-5ryIYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 50\n",
        "seq_len = 4\n",
        "input_dim = 1\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "hidden_size = 20\n",
        "\n",
        "# Data generation\n",
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "Xs, Ys, mean, std = standardize(X)\n",
        "\n",
        "# Declaration of the RNN model\n",
        "model_rnn = RNNLinear(input_dim, output_dim, hidden_size, n_layers)\n",
        "\n",
        "# Transfer the model to the proper device\n",
        "model_rnn = model_rnn.to(device)\n",
        "\n",
        "# save its initial weights\n",
        "init_rnn_weights = copy.deepcopy(model_rnn.state_dict())\n",
        "\n",
        "# Transfer the data to the proper device\n",
        "Xs = Xs.to(device)\n",
        "\n",
        "# Use the RNN to predict the output of each input sequence prior to training\n",
        "# Ensure that the inputs and output are correct\n",
        "y_pred = model_rnn(Xs)\n",
        "print(\"Size of input data: {}\".format(Xs.shape)) # (n_samples, seq_len, input_dim)\n",
        "print(\"Size of predictions: {}\".format(y_pred.shape)) # (n_samples, input_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jg8vsRduhPYH"
      },
      "source": [
        "## LSTM implementation\n",
        "\n",
        "We will now implement an LSTM using this PyTorch class [torch.nn.LSTM()](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM). Just like with the RNN, we will need to add a linear layer to transform the last recurrent state of our LSTM to have the same dimensions of our target `Y` which has size `(batch_size, input_dim)` (with `input_dim=1` as above). For additional information regarding the implementation of this class have look at this [tutorial](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net).\n",
        "\n",
        "To define the architecture of our LSTM, the following methods are to be completed:\n",
        "<ul>\n",
        "    <li>The `__init__()` method to define the different layers of our model.</li>\n",
        "<li>The `forward()` method method which uses the layers and the input variables and returns an output (this is effectively a *forward pass*). </li>\n",
        "</ul>\n",
        "\n",
        "**NB**: \n",
        "\n",
        "* Remember that only the last output of the RNN is needed, i.e., `output[-1]`.\n",
        "\n",
        "* You must ensure that the dimensions of your input data `X` matches what is required by the LSTM class [torch.nn.LSTM()](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM). \n",
        "   \n",
        "   **Hint**: this method [tensor.transpose()](https://pytorch.org/docs/stable/tensors.html?highlight=transpose#torch.Tensor.transpose) can be useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d81VVZMn3vfu"
      },
      "source": [
        "### Exercise:\n",
        "\n",
        "Complete this piece of code to implement the above described network using the [torch.nn.LSTM()](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM) class. The input arguments are the following:\n",
        "- **input_dim**: the dimension of the input data\n",
        "- **output_dim**: the dimension of the output data\n",
        "- **hiden_size**: the size of the hidden state of the RNN\n",
        "- **n_layers**: the number of layers of the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lqMaUDDVhPYH",
        "colab": {}
      },
      "source": [
        "class LSTMLinear(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, hidden_size, n_layers):\n",
        "        super(LSTMLinear, self).__init__()\n",
        "        self.LSTM = nn.LSTM(input_dim,\n",
        "                            hidden_size,\n",
        "                            n_layers)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # the input to an LSTM must be of size (seq_len, batch_size, input_dim)\n",
        "        x = x.transpose(0, 1)\n",
        "        output, _ = self.LSTM(x)\n",
        "        pred = self.linear(output[-1])\n",
        "        \n",
        "        return pred\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JucyE-Y54j2X"
      },
      "source": [
        "Here is an example of how this class can be used to predict the values of the sequences in Xs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t48-FY7d4qQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 50\n",
        "seq_len = 4\n",
        "input_dim = 1\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "hidden_size = 20\n",
        "\n",
        "# Data generation\n",
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "Xs, Ys, mean, std = standardize(X)\n",
        "\n",
        "# Declaration of the LSTM model\n",
        "model_lstm = LSTMLinear(input_dim, output_dim, hidden_size, n_layers)\n",
        "\n",
        "# Transfer the model to the proper device\n",
        "model_lstm = model_lstm.to(device)\n",
        "\n",
        "# save its initial weights\n",
        "init_lstm_weights = copy.deepcopy(model_lstm.state_dict())\n",
        "\n",
        "# Transfer the data to the proper device\n",
        "Xs = Xs.to(device)\n",
        "\n",
        "# Use the RNN to predict the output of each input sequence prior to training\n",
        "# Ensure that the inputs and output are correct\n",
        "y_pred = model_lstm(Xs)\n",
        "print(\"Size of input data: {}\".format(Xs.shape)) # (n_samples, seq_len, input_dim)\n",
        "print(\"Size of predictions: {}\".format(y_pred.shape)) # (n_samples, input_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gdha8WWWhPXp"
      },
      "source": [
        "## Splitting the data into train/validation/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6aIPUXR50aJ",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "We will use PyTorch's DataLoader objects to efficiently manipulate our data. \n",
        "\n",
        "We will generate 20,000 sequences and use 80% of those for training, 10% for validation, and 10% for testing. We can use the following functions [torch.utils.data.TensorDataset()](https://pytorch.org/docs/stable/data.html) and [torch.utils.data.DataLoader()](https://pytorch.org/docs/stable/data.html) to prepare our Dataloader.\n",
        "\n",
        "Use the following values: \n",
        "\n",
        "`seq_len = 18` \n",
        "\n",
        "`batch_size = 64`\n",
        "\n",
        "`n_samples = 25000`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "colab_type": "code",
        "id": "PzSCKyyMhPXq",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "n_samples = 25000\n",
        "seq_len = 18\n",
        "batch_size = 64\n",
        "\n",
        "# generate the data:\n",
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "\n",
        "# and standardize it:\n",
        "Xs, Ys, mean, std = standardize(X)\n",
        "\n",
        "xtrain, ytrain = Xs[:round(0.8*n_samples)], Ys[:round(0.8*n_samples)]\n",
        "xvalid, yvalid = Xs[round(0.8*n_samples):round(0.9*n_samples)], Ys[round(0.8*n_samples):round(0.9*n_samples)]\n",
        "xtest, ytest = Xs[round(0.9*n_samples):], Ys[round(0.9*n_samples):]\n",
        "\n",
        "# dataloader for the training dataset\n",
        "train_loader = DataLoader(TensorDataset(xtrain, ytrain), batch_size, shuffle=True)\n",
        "\n",
        "# do the dataloader for the validation dataset\n",
        "valid_loader = DataLoader(TensorDataset(xvalid, yvalid), batch_size, shuffle=False)\n",
        "\n",
        "# dataloader for the test dataset\n",
        "test_loader = DataLoader(TensorDataset(xtest, ytest), batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W8IdyMkZjRs7"
      },
      "source": [
        "## Training the RNN-based model\n",
        "\n",
        "Several cost functions and optimizers can be used from PyTorch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ow4dIMYT2gek"
      },
      "source": [
        "### Exercise: Defining the cost and the optimizer\n",
        "\n",
        "Recall that a cost function  $J(\\theta) = L(x, y, \\theta)$ takes as input a prediction and the target and evaluates some distance (or discrepancy) between both.  For this example, we will use the mean squared error cost which is standard for regression problems (see [torch.nn.MSELoss()](https://pytorch.org/docs/stable/nn.html)):\n",
        "\n",
        "$J(\\cdot) = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^{2}$.\n",
        "\n",
        "\n",
        "To optimize the parameters of our networks we will use the *stochastic gradient descent* (SGD) optimizer. It minimizes the cost function $J(\\theta)$ parametrized by the networks' weights $\\theta$ by updating them using the following update rule: $\\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta)$, where  $\\alpha$ is the *learning rate*. The specificity of SGD is that it will calculate the gradient $\\nabla$ using a single (or a small number of) example(s) instead of the full training data.\n",
        "\n",
        "In PyTorch we will use <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD()`</a> which is a SGD implementation. In this example, we will use a learning rate of 0.001.\n",
        "\n",
        "Complete the following piece of code by defining the MSE criterion and the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6SidKvG6hPXu",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "# Define the Criterion\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the Optimizer\n",
        "optimizer_rnn = optim.SGD(model_rnn.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pBrLb8PdhPXw"
      },
      "source": [
        "### Exercise: Training the model\n",
        "\n",
        "To train out model, we will use our `train_loader` object to iterate over our entire training sets *n_epoch* times. \n",
        "To measure progress we will store the validation cost at the end of each training *epoch*. We will use `n_epoch = 25`.\n",
        "\n",
        "Complete the following piece of code with the instructions correponding to the related comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e9a-gjjVhPXx",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "since = time.time()\n",
        "\n",
        "\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "n_epoch = 25\n",
        "\n",
        "model_rnn.load_state_dict(init_rnn_weights)\n",
        "\n",
        "print(\"Start training\")\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model_rnn.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for x, y in train_loader:  \n",
        "\n",
        "        \n",
        "        # Put tensors on device (GPU when available)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        optimizer_rnn.zero_grad()\n",
        "        \n",
        "        # Perform the Forward operation\n",
        "        outputs = model_rnn(x)\n",
        "        \n",
        "        # Calculate the loss using the criterion function\n",
        "        loss = criterion(outputs, y)\n",
        "        \n",
        "        # Perform the Backward operation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Perform the Optimization step\n",
        "        optimizer_rnn.step()\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.item()\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model_rnn.eval()\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # Iterate over valid data\n",
        "        for x, y in valid_loader:  \n",
        "        \n",
        "            # Put tensors on device (GPU when available)\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "        \n",
        "            # Perform the Forward operation\n",
        "            outputs = model_rnn(x)\n",
        "            \n",
        "            # Calculate the loss using the criterion function\n",
        "            loss = criterion(outputs, y)\n",
        "        \n",
        "            # Statistics\n",
        "            valid_loss += loss.item()\n",
        "            valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "\n",
        "\n",
        "    print(\"Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} \"\n",
        "          .format(epoch+1, (train_loss / train_n_iter), (valid_loss / valid_n_iter)))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wr4at5nVhPXz"
      },
      "source": [
        "### Visualizing training curves \n",
        "\n",
        "Visualize the training curves using a graph of the cost function vs. epochs for both the training and the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bJLu-rC_hPX1",
        "colab": {}
      },
      "source": [
        "# Save history for later\n",
        "rnn_train_loss_history = train_loss_history\n",
        "rnn_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "xaxis = range(1, n_epoch + 1)\n",
        "plt.plot(xaxis, rnn_train_loss_history, label='train-rnn')\n",
        "plt.plot(xaxis, rnn_valid_loss_history, label='valid-rnn')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vi7bHNzykdby"
      },
      "source": [
        "## Training the LSTM-based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2RUJdIy72rZu"
      },
      "source": [
        "(This cell is reproduced from the equivalent RNN one above, skip if you have read/understood that one.)\n",
        "\n",
        "### Exercise: Defining the cost and the optimizer\n",
        "\n",
        "Recall that a cost function  $J(\\theta) = L(x, y, \\theta)$ takes as input a prediction and the target and evaluates some distance (or discrepancy) between both.  For this example, we will use the mean squared error cost which is standard for regression problems (see [torch.nn.MSELoss()](https://pytorch.org/docs/stable/nn.html)):\n",
        "\n",
        "$J(\\cdot) = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^{2}$.\n",
        "\n",
        "\n",
        "To optimize the parameters of our networks we will use the *stochastic gradient descent* (SGD) optimizer. It minimizes the cost function $J(\\theta)$ parametrized by the networks' weights $\\theta$ by updating them using the following update rule: $\\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta)$, where  $\\alpha$ is the *learning rate*. The specificity of SGD is that ti will calculate the gradient $\\nabla$ using a single (or a small number of) example(s) instead of the full training data.\n",
        "\n",
        "In PyTorch we will use <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD()`</a> which is a SGD implementation. In this example, we will use a learning rate of 0.01.\n",
        "\n",
        "Complete the following piece of code by defining the MSE criterion and the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pOZIqkBbpmeo",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "# define the Criterion\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# define the optimizer\n",
        "optimizer_lstm = optim.SGD(model_lstm.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qqnTlRq8pmew"
      },
      "source": [
        "### Exercise: Training the model\n",
        "\n",
        "To train out model, we will use our `train_loader` object to iterate over our entire training sets *n_epoch* times. \n",
        "To measure progress we will store the validation cost at the end of each training *epoch*. We will use `n_epoch = 25`.\n",
        "\n",
        "Complete the following piece of code with the instructions correponding to the related comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "odcwM6DyhPYL",
        "colab": {}
      },
      "source": [
        "since = time.time()\n",
        "\n",
        "\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "model_lstm.load_state_dict(init_lstm_weights)\n",
        "\n",
        "print(\"# Start training #\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model_lstm.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for x, y in train_loader:  \n",
        "\n",
        "        \n",
        "        # Put tensors on device (GPU when available)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        optimizer_lstm.zero_grad()\n",
        "        \n",
        "        # Perform the Forward operation\n",
        "        outputs = model_lstm(x)\n",
        "        \n",
        "        # Calculate the loss using the criterion function\n",
        "        loss = criterion(outputs, y)\n",
        "        \n",
        "        # Perform the Backward operation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Perform the Optimization step\n",
        "        optimizer_lstm.step()\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.item()\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model_lstm.eval()\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # Iterate over valid data\n",
        "        for x, y in valid_loader:  \n",
        "        \n",
        "            # Put tensors on device (GPU when available)\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "        \n",
        "            # Perform the Forward operation\n",
        "            outputs = model_lstm(x)\n",
        "            \n",
        "            # Calculate the loss using the criterion function\n",
        "            loss = criterion(outputs, y)\n",
        "        \n",
        "            # Statistics\n",
        "            valid_loss += loss.item()\n",
        "            valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print(\"Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} \"\n",
        "          .format(epoch+1, (train_loss / train_n_iter), (valid_loss / valid_n_iter)))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aMEzsqBaqGmh"
      },
      "source": [
        "### Visualizing training curves \n",
        "\n",
        "Visualize the training curves using a graph of the cost function vs. epochs for both the training and the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Rw1tIfMhPYN",
        "colab": {}
      },
      "source": [
        "# Save history for later\n",
        "lstm_train_loss_history = train_loss_history\n",
        "lstm_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "xaxis = range(1, num_epochs + 1)\n",
        "plt.plot(xaxis, lstm_train_loss_history, label='train-lstm')\n",
        "plt.plot(xaxis, lstm_valid_loss_history, label='valid-lstm')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7PCowiQxpcHv"
      },
      "source": [
        "## Analyzing the results\n",
        "\n",
        "We will now compare the RNN and the LSTM using their performance on train/validation and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KaoQgOOZKL98"
      },
      "source": [
        "### Exercise: Comparing training curves\n",
        "\n",
        "\n",
        "Compare the training curves using a graph of the cost function vs. epochs for both the training and the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yl8hcNhnKKOS",
        "colab": {}
      },
      "source": [
        "# Plot training and validation curve\n",
        "xaxis = range(1, num_epochs + 1)\n",
        "\n",
        "plt.plot(xaxis, rnn_train_loss_history, label='train-rnn')\n",
        "plt.plot(xaxis, rnn_valid_loss_history, label='valid-rnn')\n",
        "\n",
        "plt.plot(xaxis, lstm_train_loss_history, label='train-lstm', linestyle='--')\n",
        "plt.plot(xaxis, lstm_valid_loss_history, label='valid-lstm', linestyle='--')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BhGBAmabKr8A"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "1. Which network works best according to validation? Why is that? \n",
        "2. Assume you want to increase the gap between the RNN and LSTM performances. How would you modify the data?\n",
        "\n",
        "\n",
        "**Possible answers**\n",
        "1. Being a simple toy problem, both the architectures converge to similar results. Still, LSTM should achieve slightly better performances. On the other hand, LSTM is slower to converge (given the architecture is more complex).\n",
        "2. The gap between LSTM and RNN should increase if we increase the length of the sequence. With long sequences, the ability of LSTM to better capture long-term dependencies should make a difference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SeWGSfEOLdsM"
      },
      "source": [
        "### Exercise: Evaluating test error\n",
        "\n",
        "Complete the following piece of code in order to evaluate and print the mean squared error on the test set for both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "72z2Xob3MRYB",
        "colab": {}
      },
      "source": [
        "# Put sequence data on device (GPU when available)\n",
        "xtest = xtest.to(device)\n",
        "\n",
        "# Put target data on device (GPU when available)\n",
        "ytest = ytest.to(device)\n",
        "\n",
        "# Predict the value of the sequence data using the RNN-based model\n",
        "ypred_rnn = model_rnn(xtest)\n",
        "\n",
        "# Predict the value of the sequence data using the LSTM-based model\n",
        "ypred_lstm = model_lstm(xtest)\n",
        "\n",
        "# Compute the loss of the RNN-based model using the criterion function\n",
        "loss_test_rnn = criterion(ypred_rnn, ytest)\n",
        "\n",
        "# Compute the loss of the LSTM-based model using the criterion function\n",
        "loss_test_lstm = criterion(ypred_lstm, ytest)\n",
        "\n",
        "\n",
        "print(\"The RNN's test mean squared error is %2.3f\" % float(loss_test_rnn))\n",
        "print(\"The LSTM's test mean squared error is %2.3f\" % float(loss_test_lstm))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hPZH71eCpcH0"
      },
      "source": [
        "### Exercise: Further exploring the results\n",
        "\n",
        "To gain insights into our models we will inspect their outputs (i.e., their predictions before calculating the cost). To help us we will define a `print_sequence()` function. \n",
        "\n",
        "This function takes as input the tensors X and Y, samples a particular entry (sequence), prints the entry and prints the absolute difference between the true Y and the predicted Y.\n",
        "\n",
        "Recall that we have standardized our examples. For these visualizations we wish to use the original data pre-standardization. To do so, we use `mean` + `std` such that $xtest\\_unstd = xtest*std + mean$ et $ytest\\_unstd = ytest*std+seq\\_len*mean$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QfCvsGXNpcH2",
        "colab": {}
      },
      "source": [
        "def print_sequence(X, Y, idx=0):\n",
        "    \"\"\"Print ground truth sum and predicted values.\n",
        "    Args:\n",
        "      X: torch.Tensor.\n",
        "      Y: torch.Tensor.\n",
        "      idx: index of the sequence (entry) to use.\n",
        "    \"\"\"\n",
        "    x = X[idx].numpy()\n",
        "    y = Y[idx].numpy()\n",
        "    for i, xi in enumerate(x):\n",
        "        if i==0:\n",
        "            string = str(xi[0]) \n",
        "        else:\n",
        "            string += \" + \" + str(xi[0])\n",
        "            \n",
        "    print(\"Sequence: \", string)\n",
        "    print(\"Prediction: \", str(y[0]))\n",
        "    print(\"Ground truth value: \", str(np.sum(x)))\n",
        "    diff = abs(np.sum(x)-y[0])\n",
        "    print(\"Absolute error between X[{a}] et Y[{a}]: {b}\".format(a=idx, b=diff))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D196dQEAoxu",
        "colab_type": "text"
      },
      "source": [
        "Complete the following piece of code by unstandardizing `xtest` and `ypred_rnn` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_aDw139hpcH_",
        "colab": {}
      },
      "source": [
        "xtest = xtest.cpu()\n",
        "ypred_rnn = ypred_rnn.cpu()\n",
        "\n",
        "# Unstandardize  xtest\n",
        "xtest_unstd = xtest*std + mean\n",
        "\n",
        "# Unstandardize  the predictions of the RNN-based model on xtest\n",
        "ypred_unstd = ypred_rnn*std + seq_len*mean\n",
        "\n",
        "idx = np.random.randint(len(ytest))\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Example RNN:\")\n",
        "print(\"\")\n",
        "\n",
        "print_sequence(xtest_unstd.detach(), ypred_unstd.detach(), idx=idx)\n",
        "\n",
        "\n",
        "\n",
        "xtest = xtest.cpu()\n",
        "ypred_lstm = ypred_lstm.cpu()\n",
        "\n",
        "xtest_unstd = xtest*std + mean\n",
        "ypred_unstd = ypred_lstm*std + seq_len*mean\n",
        "\n",
        "print(\"\")\n",
        "print(\"Example LSTM:\")\n",
        "print(\"\")\n",
        "\n",
        "print_sequence(xtest_unstd.detach(), ypred_unstd.detach(), idx=idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tAM5UeILQb5l"
      },
      "source": [
        "** Bonus exercise**\n",
        "\n",
        "* Redo the above analysis but instead of summing the input values try other operations (e.g., product)\n",
        "* Compare the performance of the RNN and the LSTM for sequences of different length (i.e., change `seq_len`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y1jHLBHErzbW"
      },
      "source": [
        "---\n",
        "# Task 2: Neural language model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h7kMH3bm1ZDz"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys_geN7cCxO3",
        "colab_type": "text"
      },
      "source": [
        "The objective of the second part of the tutorial is to learn about text generation using recurrent neural networks. In particular, we will train a recurrent neural network using a small amount of textual data written by [Shakespeare]https://en.wikipedia.org/wiki/William_Shakespeare). Once this model is trained, we will use it to generate new text in the style of Shakespeare.\n",
        "\n",
        "**Note** that people often use the term RNN even when training an LSTM. RNN has become the generic term regardless of the type (in addition to LSTMs, there are other variants that are commonly used such as [Gated Recurrent Units (GRUs)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)).\n",
        "\n",
        "The model we will use is a bit more complex than the one for task 1. Contrary to the previous section, all code is provided (i.e., there are no exercises to complete). We suggest you go through all of the code to ensure that you understand both its logic as it relates to text processing but also how to design and train an RNN for text generation. The code could be relatively easily adapted to other tasks that you may care about. \n",
        "\n",
        "Further, this notebook also contains some of the data pre-processing steps, in particular the ones that take our dataset and arranges it such that it can be used to train a neural language model based on LSTMs.  \n",
        "\n",
        "Happy generation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-TECjbTm3L20"
      },
      "source": [
        "## A bit more context to understand language modelling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcPO8bUVC6bH",
        "colab_type": "text"
      },
      "source": [
        "A sequence of words $\\mathbf{s}$ can be represented as a sequence of $N$ discrete symbols (or lexical tokens) such that $\\mathbf{s} = (w_{1}, \\dots, w_{N})$, where $w_{t}$ is a word or a punctation mark. Each symbol can be represented by an integer corresponding to its index in the vocabulary $V$. $V$ contains all symbols for a particular task (the vocabulary is usually built from the dataset we use for a task). The objective of a language model is to estimate the (joint) probability of a sequence $p(\\mathbf{s}) = p(w_{1}, \\dots, w_{N})$, which can be decomposed as a product of conditional probabilities such that:\n",
        "\n",
        "\\begin{equation}\n",
        "  p(\\mathbf{s}) = \\prod^{N}_{t=1} p(w_{t} | w_{1}, \\dots, w_{t-1}).\n",
        "\\end{equation}\n",
        "(this is also known as the chain rule in probability theory)\n",
        "\n",
        "This is important for modelling. In particular, instead of modelling the joint distribution directly, we can \"simply\" model each conditional. That is we can model the probability of the next word given all previous words ($p(w_{t} | w_{<t})$). Language models do just that and are widely used across numerous applications (including in automatic translation, speech recognition, and information retrieval). Note that this can be understood as a multi-class classification problem where the classes corresponds to the different words.\n",
        "\n",
        "However, modelling each conditional is not easy. Instead we perform an approximation which makes the problem easier. The intuition behind the approximation is that instead of conditioning of the whole history we condition on a smaller history ($w_{t-1}, w_{t-2}, \\ldots, w_{t-n}$) to predict the next word ($w_{t}$). This is also known as a $n$-order Markov assumption. Mathematically it is:\n",
        "\\begin{equation}\n",
        "  p(w_{t} | w_{1}, \\dots, w_{t-1}) \\approx p(w_{t} | w_{t-n}, \\dots, w_{t-1}).\n",
        "\\end{equation}\n",
        "\n",
        "In the next paragraph we explain how to model the above conditionals using a recurrent neural network. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LShDE1ANDu9p",
        "colab_type": "text"
      },
      "source": [
        "### Modeling using RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn-Jr6iuDyaQ",
        "colab_type": "text"
      },
      "source": [
        "The architecture of the neural language model we use is an LSTM networks which will learn at each timestep a conditional distribution of the next word using a number of previous words.\n",
        "To train its parameters, we first need to set the maximum number $n$ of previous words to consider (`seq_len` in the code)--this is the effective size of your history--for training $p(w_{t} | w_{t-n}, \\dots, w_{t-1})$. The input to the LSTM at each time step is: \n",
        "- the $t^{th}$ word $w_t$ encoded using its *word embedding* (see below); \n",
        "- the recurrent state ($\\mathbf{h}_{t-1}$); and \n",
        "- the state of the memory at the previous timestep ($\\mathbf{c}_{t-1}$).\n",
        "\n",
        "The output of the LSTM at each step is the next word $w_{t+1}$. That is, we train the LSTM to predict the next word at each step (in details the LSTM will actually predict the probability of that next word). This also implies that once this model is trained, we will be able to use it to generate text (we will simply pick the word with the highest probability and feed it as the input to the next step). This type of model is commonly known as an *LSTM-based neural network model*. Its architecture is shown below:\n",
        "\n",
        "![alt-text](https://github.com/nextai-mtl/tech-2019/blob/master/images/autoregressive_english.png?raw=true)\n",
        "\n",
        "\n",
        "To compute the probability over all next words, we simply use a *softmax* activation function. The softmax function returns a normalized $|V|$-dimensional vector, where each entry corresponds to a single word in the vocabulary. Each entry can be understood as the \"probability\" that the next word should the word at this index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOYS6x0rDDvV",
        "colab_type": "text"
      },
      "source": [
        "### Word Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTg07dHGDHAr",
        "colab_type": "text"
      },
      "source": [
        "The main question that remains is how exactly to encode the words at each timestep. \n",
        "Since the input data to a neural network must be encodable in a matrix, each symbol (word) $w_t$ in the vocabulary can be represented as a *one-hot* vector $\\mathbf{x}_i$ which is a vector of zeros with a single 1 at the position of the index of this word in the vocabulary. Thus, these *one-hot* vectors belong to $ \\mathbb R^{|V|}$ where $|V|$ is the size of the vocabulary, that is, the number of words in the vocabulary. These one-hot vectors are multiplied by a weight matrix $\\mathbf{E} \\in \\mathbb R^{|V| \\times d_{e}}$. This matrix is learned and is known as the *embedding matrix*, $\\mathbf{E} \\in \\mathbb R^{|V| \\times d_{e}}$, it effectively encodes words in a continuous representation: $\\mathbf{w}_{t} \\in \\mathbb R^{d_{e}}$.\n",
        "\n",
        "Using word embeddings, a sequence of words can therefore be represented a sequence of size-$N$ vectors $\\mathbf{s} = (\\mathbf{w}_{1}, \\dots, \\mathbf{w}_{N})$. Each line $i$ of this matrix $\\mathbf{E}$ is a representation in $d_{e}$ dimensions of the $i$'th word in the vocabulary $V$. As we said above, these representations are often called *word embeddings*. When they are learned from a large enough dataset, they can represent semantic similarity. [For more information regarding word embeddings](http://ruder.io/word-embeddings-1/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u1Sh2OG46_Ln"
      },
      "source": [
        "## Utility functions for processing text and data structuring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sf7luc-Di7qO"
      },
      "source": [
        "To train our neural language model, we will use a dataset (corpus) of the works of Shakespeare (we used a subset from this link [https://norvig.com/ngrams/] which was cleaned, tokenized and standardized) available in the file `shakespeare_top20K.txt`.\n",
        "\n",
        "To obtain reasonable results in practice we should train a language model using a very-large quantity of text. In this tutorial, we will use a relatively small corpus of 20000 sentences, 159884 tokens, and a vocabulary of 12354 tokens. A token is a lexical unit separated by a whitespace on each side in the text. In our case, a token is a word, a number or a punctuation mark. The vocabulary is the set of all tokens in a corpus. (Of course, our code generalizes to larger datasets.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yo7yRlDyK7DA",
        "colab": {}
      },
      "source": [
        "# Clone the git repo to obtain the data\n",
        "!git clone https://github.com/nextai-mtl/tech-2019.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0pjv24-kr528",
        "colab": {}
      },
      "source": [
        "START_VOCAB = [\"_UNK\"]\n",
        "UNK_ID = 0\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-hkdM7Oe1Ei",
        "colab_type": "text"
      },
      "source": [
        "The function `create_vocabulary` takes as input:\n",
        "- **corpus_path**: path to a corpus data\n",
        "- **vocab_path**: the path where a vocabulary from the provided corpus will be created\n",
        "- **max_vocab_size**: the maximum number of words of the vocabulary\n",
        "\n",
        "It creates a vocabulary composing of at most `max_vocab_size` of the frequent tokens within the corpus data. This vocabulary is saved under the `vocab_path` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJEgI8vtf1NQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vocabulary(corpus_path, vocab_path, max_vocab_size=1e5):\n",
        "    \"\"\"Create and save the vocabulary of a corpus.\"\"\"\n",
        "    vocab = {}\n",
        "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip().split()\n",
        "            for token in tokens:\n",
        "                if token in vocab:\n",
        "                    vocab[token] += 1\n",
        "                else:\n",
        "                    vocab[token] = 1\n",
        "    vocab_list = START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
        "    if len(vocab_list) > max_vocab_size:\n",
        "        vocab_list = vocab_list[:max_vocab_size]\n",
        "    with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for token in vocab_list:\n",
        "              f.write(token + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCWk261tf6HE",
        "colab_type": "text"
      },
      "source": [
        "The function `initialize_vocabulary` takes as input:\n",
        "- **vocab_path**: the path to the file containing the vocabulary\n",
        "\n",
        "It returns a tuple of two elements corresponding respectively to:\n",
        "- **vocab**: a dictionnary `Token:Index` associating an index to each token in the vocabulary\n",
        "- **rev_vocab**: a list of the (unique) tokens in the vocabulay associating each index to a token such that $vocab [rev\\_vocab[i]] = i$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLBHg0UJhYHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_vocabulary(vocab_path):\n",
        "    \"\"\"Initialize the vocabulary.\"\"\"\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            rev_vocab = [line.strip() for line in f.readlines()]\n",
        "        vocab = dict([(w, i) for (i, w) in enumerate(rev_vocab)])\n",
        "        return vocab, rev_vocab\n",
        "    else:\n",
        "        raise ValueError(\"Vocabulary file {} not found.\".format(vocab_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1HaFChPhjGe",
        "colab_type": "text"
      },
      "source": [
        "The function `read_corpus` takes as input:\n",
        "- **corpus_path**: path to a corpus data\n",
        "\n",
        "It returns the corpus data as a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxCliC-iicku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_corpus(corpus_path):\n",
        "    \"\"\"Read and convert a corpus in a list of tokens.\"\"\"\n",
        "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        corpus = f.read().split()\n",
        "    return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBsQFZ0OihKU",
        "colab_type": "text"
      },
      "source": [
        "The function `corpus_to_token_ids` takes as input:\n",
        "- **corpus**: the corpus as a list of tokens\n",
        "- **vocab**: the vocabulary as a distionnary of the form `token:index`.\n",
        "\n",
        "It returns a list of token-ids corresponding to the list of tokens as defined by the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvWwHUN5kDFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corpus_to_token_ids(corpus, vocab):\n",
        "    \"\"\"Convert a corpus in token-ids.\"\"\"\n",
        "    token_ids = [vocab.get(token, UNK_ID) for token in corpus]\n",
        "    return token_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqPMutYMkghO",
        "colab_type": "text"
      },
      "source": [
        "The function `batch_data` takes as input:\n",
        "- **data**: list of token_ids\n",
        "- **batch_size**: the batch size\n",
        "\n",
        "It returns a structured version of the data in `batch_size` continuous sequences. The returned data has a shape `N* x batch_size` where `N* = int(len(data) / batch_size)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf8S_HKMmhcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_data(data, batch_size):\n",
        "    \"\"\"Structure the data in bath_size continuous sequences.\"\"\"\n",
        "    \n",
        "    n_batch = len(data) // batch_size\n",
        "    data = np.array(data[:n_batch*batch_size])\n",
        "    data = data.reshape(batch_size, -1).T\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEbft5O6mqpt",
        "colab_type": "text"
      },
      "source": [
        "The function `detach_hidden` takes as input:\n",
        "- **hidden**: hidden state of an RNN\n",
        "\n",
        "It transforms the data from this hidden state into a new Tensor with same values but where the computational history are lost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_sdH5sXVsDgU",
        "colab": {}
      },
      "source": [
        "def detach_hidden(hidden):\n",
        "    \"\"\"Transform the data from the hidden states of the LSTM\n",
        "       into a new Tensor with require_grad=False.\"\"\"\n",
        "    return tuple(h.detach() for h in hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw0bPi3kngpr",
        "colab_type": "text"
      },
      "source": [
        "Here is the piece of code where the corpus data are read, the vocabulary is created and the corpus data is structured in continuous sequences of `batch_size` for training purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_mmiW8j9bmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and initialize the vocabulary\n",
        "corpus_path = \"tech-2019/data/shakespeare_top20K.txt\"\n",
        "vocab_path = \"vocab.txt\"\n",
        "\n",
        "create_vocabulary(corpus_path, vocab_path)\n",
        "vocab, rev_vocab = initialize_vocabulary(vocab_path)\n",
        "\n",
        "# Read the training corpus\n",
        "corpus = read_corpus(corpus_path)\n",
        "token_ids = corpus_to_token_ids(corpus, vocab)\n",
        "\n",
        "# Structure the corpus in continuous sequences of batch_size for training\n",
        "batch_size = 10\n",
        "data = batch_data(token_ids, batch_size)\n",
        "data = torch.LongTensor(data).to(DEVICE)\n",
        "\n",
        "print(\"Number of tokens in the corpus: {}\"\n",
        "      .format(len(corpus)), end=\"\\n\\n\")\n",
        "print(\"Size of vocabulary: {}\"\n",
        "      .format(len(vocab)), end=\"\\n\\n\")\n",
        "print(\"Top 20 most frequent tokens in the corpus: \\n{}\"\n",
        "      .format(rev_vocab[1:21]), end=\"\\n\\n\")\n",
        "print(\"First sentence in the corpus in text format:\\n{}\"\n",
        "      .format(\" \".join(corpus[:31])), end=\"\\n\\n\")\n",
        "print(\"First sentence in the corpus in token-id format:\\n{}\"\n",
        "      .format(token_ids[:31]), end=\"\\n\\n\")\n",
        "print(\"Test conversion from token-ids to text using rev_vocab:\\n{}\"\n",
        "      .format(\" \".join([rev_vocab[i] for i in token_ids[:31]])), end=\"\\n\\n\")\n",
        "print(\"Structure the training data (note that the first sentence \"\n",
        "      \"is in the first colon):\\n{}\".format(data[:20]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3G4jM1en_Og",
        "colab_type": "text"
      },
      "source": [
        "In the following block, we are splitting the data into training and validation datasets and we create the corresponding dataloaders. We use $10\\%$ of the data for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E2EXx8HBsZAY",
        "colab": {}
      },
      "source": [
        "# Split ensembles training/validation and create DataLoaders\n",
        "X = data[:-1]\n",
        "Y = data[1:]\n",
        "\n",
        "n_valid = round(data.size(0) * 0.1)\n",
        "train_set = TensorDataset(X[:(data.size(0)-n_valid)], Y[:(data.size(0)-n_valid)])\n",
        "valid_set = TensorDataset(X[-n_valid:], Y[-n_valid:])\n",
        "\n",
        "seq_len = 40\n",
        "train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)\n",
        "valid_loader = DataLoader(valid_set, batch_size=seq_len, shuffle=False)\n",
        "\n",
        "print(\"Training set: {} sequences of length {} and {} minibatches\"\n",
        "      .format(len(train_set), seq_len, len(train_loader)))\n",
        "print(\"Validation set: {} sequences of length {} and {} minibatches\"\n",
        "      .format(len(valid_set), seq_len, len(valid_loader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IAgSaVwmVxrt"
      },
      "source": [
        "## Implementing the model\n",
        "\n",
        "Below you will find the code of an *LSTM-based neural language model* as described in the **A bit more context to understand language modelling** Section.\n",
        "\n",
        "At each timestep, the input variables of the LSTM are:\n",
        "<ol>\n",
        "    <li> a minibatch of sequences of token-ids (i.e., sequences of indices where each index represents the position of a token in the vocabulary);</li>\n",
        "    <li> tuples $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ corresponding to recurrent states and memory states  $(\\mathbf{h}_{T}, \\mathbf{c}_{T})$ from the previous minibatch  (except for the first minibatch where we simply initialize these variables to 0.0 with the `init_hidden()` function).\n",
        "</ol>\n",
        "\n",
        "Each sequence of token-ids is transformed in sequences of *word embeddings* by indexing the *word embeddings* created by the class [torch.nn.Embedding()](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) which is a matrix of parameters of dimension $|V| \\times d_{e}$, where $|V|$ is the vocabulary size (`vocab_size`) and $d_{e}$ is the dimension of a *word embedding* (`embedding_size`).\n",
        "\n",
        "We will also apply dropout on the input *word embeddings* as well as on the output layer to provide regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0YMiwftasb4g",
        "colab": {}
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"LSTM based language neural model\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, input_size, hidden_size, n_layers=1, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          vocab_size: size of the vocabulary.\n",
        "          input_size: size of the word embeddings.\n",
        "          hidden_size: size of the hiden states of the LSTM.\n",
        "          n_layers: number of layers of the LSTM (default: 1).\n",
        "          dropout: if non-zero, uses a layer of dropout at the input at at the output of \n",
        "                    the LSTM introduit une couche dropout à l'entrée et à la sortie,\n",
        "                    where the input sets the value of the dropout (default: 0.5).\n",
        "        \"\"\"        \n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, input_size)\n",
        "        self.lstm = nn.LSTM(input_size,\n",
        "                            hidden_size,\n",
        "                            n_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embeddings = self.dropout(self.embeddings(input))\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "        output = self.dropout(output)\n",
        "        result = self.linear(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return result.view(output.size(0), output.size(1), result.size(1)), hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "        init.uniform_(self.embeddings.weight, -0.1, 0.1)\n",
        "        init.xavier_uniform_(self.linear.weight, init.calculate_gain(\"linear\"))\n",
        "        init.constant_(self.linear.bias, 0)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize the values of the hidden state and the LSTM cell to zero.\n",
        "        Args:\n",
        "          batch_size: size of a mini-batch at one time-step\n",
        "          \n",
        "        Returns:\n",
        "          hidden: hidden state h_t and the cell c_t à t=0 initialized to 0, \n",
        "                  ((n_layers, batch_size, hidden_size),\n",
        "                   (n_layers, batch_size, hidden_size)).\n",
        "        \"\"\"\n",
        "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE),\n",
        "                  torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE))\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oPPkC_0GV3iu"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "The training procedure is very similar to the procedure used in Task 1. The key elements that differ are: \n",
        "\n",
        "* We use dropout on the input *word embeddings* as well as on the output layer of the LSTM. The small size of our data makes regularization important. In particular, for language models based on recurrent neural networks, we suggest using a fairly large droput value.\n",
        "* We change the learning rate through training. We begin with a fairly high one (see `learning_rate`) and divide it by 10 as a function of the validation error at the end of each *epoch*. In particular we use this class:  [torch.optim.lr_scheduler.ReduceLROnPlateau()](https://pytorch.org/docs/stable/optim.html?highlight=plateau#torch.optim.lr_scheduler.ReduceLROnPlateau).\n",
        "* To mitigate the problem of *exploding gradient* we use a *gradient-clipping* approach and normalize the norm of the gradient using this function [torch.nn.utils.clip_grad_norm_()](https://pytorch.org/docs/stable/nn.html?highlight=clip#torch.nn.utils.clip_grad_norm_).\n",
        "* We initialize to 0.0 the values of the tuple containing the recurrent state and the hidden memory state $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ using the `init_hidden()` function. We do this only once at the beginning of each *epoch* and we propagate the new values of hidden through all training minibatches. In other words, the data has beeen structured using the `batch_data()` function such that we can initialize `hidden` $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ of each sequence of minibatch using $(\\mathbf{h}_{T}, \\mathbf{c}_{T})$ of the previous minibatch. Using this method implies that we cannot shuffle the order of the sequences at each training *epoch* (i.e., `train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)`).\n",
        "* Our cost function is the cross-entropy [torch.nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss) which is standard for multi-class classification problems.\n",
        "* We will use *[perplexity](https://en.wikipedia.org/wiki/Perplexity)* as a way to measure the quality of our model. The intuition is that a good model should not be perplex when evaluating new data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "winuGw1rsuOe",
        "colab": {}
      },
      "source": [
        "# Construct the model\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 300\n",
        "hidden_size = 400\n",
        "n_layers = 1\n",
        "dropout = 0.65\n",
        "model = LanguageModel(vocab_size, embedding_size, hidden_size, n_layers, dropout).to(DEVICE)\n",
        "print(\"Number of parameters in the model:\", sum(param.nelement() for param in model.parameters()))\n",
        "\n",
        "# Cost and optimization functions\n",
        "learning_rate = 10\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "# Training the model\n",
        "n_epochs = 20\n",
        "max_grad_norm = 1\n",
        "\n",
        "print(\"Training the model for {} epochs of {} minibatches\".format(n_epochs, len(train_loader)))\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        # Separate the hidden nodes from the computational graph\n",
        "        hidden = detach_hidden(hidden)\n",
        "        \n",
        "        # Reinitialize the gradient\n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        # Forward pass\n",
        "        y_pred, hidden = model(x, hidden)\n",
        "        \n",
        "        # Calculate the loss\n",
        "        loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))\n",
        "        \n",
        "        # Calculate the gradient\n",
        "        loss.backward()\n",
        "        \n",
        "        # Normalise the gradient to prevent gradient exploding\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        \n",
        "        # Update model parameters\n",
        "        optimizer.step()        \n",
        "        \n",
        "        # Accumulate total loss\n",
        "        train_loss += len(x) * loss.item()\n",
        "    \n",
        "    # Eval the model using the validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "        for x, y in valid_loader:\n",
        "            y_pred, hidden = model(x, hidden)\n",
        "            loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))\n",
        "            valid_loss += len(x) * loss.item()\n",
        "    \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    scheduler.step(valid_loss)\n",
        "\n",
        "    print(\"Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} | Perplexity = {:.2f}\"\n",
        "          .format(epoch+1, train_loss, valid_loss, np.exp(valid_loss)))\n",
        "print(\"Congratulations! You have now trained a neural language model!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tKAOoUwWV8To"
      },
      "source": [
        "## Text generation\n",
        "\n",
        "Now that we have trained a language model, we can use it to generate text as Shakespeare! \n",
        "To do so, we will randomly select the first word (i.e., a token in a vocabulary) and use that as the input token at timestep one. We will then use the output token at the first timestep as the input token at the second timestep and son on. In total we will generate `n_words`. The `smoothing` variable allows one to obtain change the diversity of the generated text. A higher value generate more diverse but often lower quality text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lLg-dbOSsuVu",
        "colab": {}
      },
      "source": [
        "# Generating sequences of words\n",
        "model.eval()\n",
        "x = torch.randint(0, vocab_size, (1, 1), dtype=torch.long, device=DEVICE)\n",
        "words = [rev_vocab[x]]\n",
        "n_words = 300\n",
        "smoothing = 0.5\n",
        "with torch.no_grad(): \n",
        "    hidden = model.init_hidden(1)\n",
        "    for i in range(n_words-1):\n",
        "        output, hidden = model(x, hidden)\n",
        "        weights = output.squeeze().div(smoothing).exp()\n",
        "        word_idx = torch.multinomial(weights, 1)\n",
        "        x.fill_(word_idx.squeeze())\n",
        "        word = rev_vocab[word_idx]\n",
        "        words.append(word)\n",
        "        if (i+1) % 15 == 0:\n",
        "            words.append(\"\\n\")\n",
        "print(\" \".join(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8HI-Elp2UhBY"
      },
      "source": [
        "What do you think of this generated text? What is remarkable is that many short sequences of words seem plausible. Longer phrases, however, usually do not make much sense. \n",
        "\n",
        "We note that both commas `,` and periods `.` appear very often in this generated text. This can be explain by the fact that they are the most frequent tokens in our (training) dataset. \n",
        "\n",
        "To obtain higher-quality texte would require training with a larger corpus. Further, the vocabulary and the style of Shakespeare may not be easiest to learn from. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lVeeLCaC29Mx"
      },
      "source": [
        "---\n",
        "## References\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "https://arxiv.org/abs/1803.08240"
      ]
    }
  ]
}
